## Performance Benchmarks

> ⚠️ NOTE:
> The benchmarks below are **placeholders** and describe the **intended**
> load-testing scenarios.
> 
> Actual performance numbers must be generated by running Locust in the
> target environment (local / CI / staging).
> This document intentionally avoids fabricated metrics.

### Planned Load Test Scenarios

| Concurrent Users | Endpoint Tested | Description |
|------------------|-----------------|-------------|
| 1                | POST /api/v1/scan | Baseline single-user scan |
| 10               | POST /api/v1/scan | Moderate concurrent load |
| 50               | POST /api/v1/scan | High concurrency stress test |

### Metrics to Capture Per Scenario

The following metrics are expected from Locust runs:

- Average response time
- p50 / p95 / p99 latency
- Requests per second (RPS)
- Error rate (%)
- Failed request count

---

## Bottlenecks (Observed / Expected)

Based on application architecture and integration test behavior, the
following components are expected bottlenecks under load:

### 1. Static Analysis Execution
- Slither execution is CPU-bound and runs per scan
- Contract size directly impacts analysis time
- Dominant contributor to request latency

### 2. Database Writes
- Each scan performs synchronous INSERT operations
- JSON fields (`severity_breakdown`) increase serialization cost
- Commit latency increases under concurrent writes

---

## Next Steps

To convert this document into **measured benchmarks**:

1. Run Locust with:
   - 1 user
   - 10 users
   - 50 users
2. Export Locust statistics (CSV or Web UI)
3. Replace this document with measured values
4. Re-evaluate bottlenecks using real latency data

